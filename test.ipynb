{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s13-d21_252_452 [[-0.02507035  0.01792968 -0.00489204 ...  0.06062402  0.02171551\n",
      "   0.03321327]]\n",
      "s13-d21_471_627 [[-0.0141677   0.01069785 -0.00177013 ...  0.03334549  0.00467701\n",
      "   0.01666418]]\n",
      "s13-d21_627_686 [[-0.02699237  0.01298359 -0.00726358 ...  0.03505049 -0.00138477\n",
      "   0.00839384]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nv = np.array(([1, 2, 3, 4], [5, 6, 7, 8]))\\ns = np.array(([1, 2, 3, 4], [5, 6, 7, 8]))\\nvv = np.reshape(np.tile(v, [2, 1]), [2, 2, 4])\\nss = np.reshape(np.tile(s, [1, 2]), [2, 2, 4])\\n\\nfor i in range(vv.shape[0]):\\n    print(i)\\n    print(vv[i])\\n    print(ss[i])\\n\\n#print(y)\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "#train_csv_path = \"E:/File/Deep_Learning/model/TALL-master/exp_data/TACoS/train_clip-sentvec.pkl\"\n",
    "test_csv_path = \"./exp_data/train_sentence.pkl\"\n",
    "#train_s = pickle.load(open(train_csv_path, 'rb'), encoding=\"latin1\")\n",
    "test_s = pickle.load(open(test_csv_path, 'rb'), encoding=\"latin1\")\n",
    "\n",
    "#train = [i[1] for i in train_s]\n",
    "test = [k for k, v in test_s.items()]\n",
    "\n",
    "#for i in train_s:\n",
    "#    sent_name = i[0]\n",
    "#    sent_vecs = i[1]\n",
    "#    print(sent_name, len(sent_vecs))\n",
    "#sent_name = train_s[7][0]\n",
    "#print(sent_name)\n",
    "#sent_vecs = train_s[7][1]\n",
    "#for vec in sent_vecs:\n",
    "#    print(vec)\n",
    "\n",
    "#print(len(train))\n",
    "#print(len(test))\n",
    "\n",
    "#print(train_s[0])\n",
    "#print(train_s[1])\n",
    "print(test[1], test_s[test[1]][0])\n",
    "print(test[2], test_s[test[2]][0])\n",
    "print(test[3], test_s[test[3]][0])\n",
    "\"\"\"\n",
    "v = np.array(([1, 2, 3, 4], [5, 6, 7, 8]))\n",
    "s = np.array(([1, 2, 3, 4], [5, 6, 7, 8]))\n",
    "vv = np.reshape(np.tile(v, [2, 1]), [2, 2, 4])\n",
    "ss = np.reshape(np.tile(s, [1, 2]), [2, 2, 4])\n",
    "\n",
    "for i in range(vv.shape[0]):\n",
    "    print(i)\n",
    "    print(vv[i])\n",
    "    print(ss[i])\n",
    "\n",
    "#print(y)\n",
    "\"\"\"\n",
    "# c3d feature\n",
    "#y = np.load('crop_mean.npy')\n",
    "\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ctrl_model\n",
    "\n",
    "train_feature_dir = \"E:/File/VS Code/DataSet/TACOS/Interval64_128_i3d_mixed5/\"\n",
    "test_feature_dir = \"E:/File/VS Code/DataSet/TACoS/Interval128_i3d_mixed5/\"\n",
    "train_csv_path = \"./exp_data/train_sentence.pkl\"\n",
    "test_csv_path = \"./exp_data/test_sentence.pkl\"\n",
    "\"\"\"\n",
    "train_feature_dir = \"E:/File/VS Code/DataSet/TACOS/Interval64_128_256_512_overlap0.8_c3d_fc6/\"\n",
    "test_feature_dir = \"E:/File/VS Code/DataSet/TACoS/Interval128_256_overlap0.8_c3d_fc6/\"\n",
    "train_csv_path = \"./exp_data/TACoS/train_clip-sentvec.pkl\"\n",
    "test_csv_path = \"./exp_data/TACoS/test_clip-sentvec.pkl\"\n",
    "\"\"\"\n",
    "model = ctrl_model.CTRL_Model(train_feature_dir, train_csv_path, test_feature_dir, test_csv_path, 50)\n",
    "\n",
    "#Reading testing data list from ./exp_data/TACoS/train_clip-sentvec.pkl\n",
    "#10146 clip-sentence pairs are readed\n",
    "#26963 iou clip-sentence pairs are readed\n",
    "#Reading testing data list from ./exp_data/TACoS/test_clip-sentvec.pkl\n",
    "#4083 pairs are readed\n",
    "#Max number of clips in a movie is 242\n",
    "#sliding clips number: 15881"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model parameters...\n",
      "Compiling encoders...\n",
      "Loading tables...\n",
      "Packing up...\n",
      "14.885078430175781\n",
      "18.463918685913086\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import skip_thoughts.skipthoughts as skipthoughts\n",
    "\n",
    "encoder = skipthoughts.Encoder(skipthoughts.load_model())\n",
    "sent_feat = []\n",
    "query_sent = ['the person set up for his kitchen activities', 'the person wiped the cucumber', \n",
    "                'the person wiped the cucumber', 'the person wiped the cucumber',\n",
    "                'the person wiped the cucumber', 'the person wiped the cucumber',\n",
    "                'the person wiped the cucumber', 'the person wiped the cucumber',\n",
    "                'the person wiped the cucumber', 'the person wiped the cucumber']\n",
    "\n",
    "start = time.time()\n",
    "#for i in range(len(query_sent)):\n",
    "query_vec = encoder.encode(query_sent)\n",
    "print(time.time() - start)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(len(query_sent)):\n",
    "    query_vec = encoder.encode([query_sent[i]])\n",
    "print(time.time() - start)\n",
    "\n",
    "#for i in range(len(query_vec)):\n",
    "#    print(query_vec[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "train_dir = \"E:/File/VS Code/DataSet/TACOS/Interval64_128_256_512_overlap0.8_c3d_fc6/\"\n",
    "test_dir = \"E:/File/VS Code/DataSet/TACoS/Interval128_256_overlap0.8_c3d_fc6/\"\n",
    "train_feature_dir = \"E:/File/VS Code/DataSet/TACOS/Interval64_128_i3d_mixed5/\"\n",
    "test_feature_dir = \"E:/File/VS Code/DataSet/TACoS/Interval128_i3d_mixed5/\"\n",
    "\n",
    "clip_name = 's17-d69.avi_1_65.npy'\n",
    "movie_clip_name = 's17-d69_0_64.npy'\n",
    "\n",
    "feature = np.load(os.path.join(train_dir, clip_name))\n",
    "\n",
    "print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n",
      "[1 0 4]\n",
      "[2 0 5]\n",
      "[3 0 6]\n",
      "[4 0 7]\n",
      "(1, 12)\n",
      "[1 0 4 2 0 5 3 0 6 4 0 7]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "context_size = 1\n",
    "\n",
    "left = np.array([[1, 2, 3, 4]]) # [1, 4]\n",
    "data = np.array([0, 0, 0, 0]) # [4]\n",
    "right = np.array([[4, 5, 6, 7]]) # [1, 4]\n",
    "\n",
    "left = np.transpose(left, [1,0]) # [4, 2]\n",
    "right = np.transpose(right, [1,0]) # [4, 2]\n",
    "\n",
    "comb_feat = np.column_stack((left, data, right))\n",
    "print(comb_feat.shape)\n",
    "\n",
    "for i in comb_feat: print(i)\n",
    "\n",
    "featmap = np.reshape(comb_feat, [1, comb_feat.shape[0] * (2 * context_size + 1)])\n",
    "print(featmap.shape)\n",
    "\n",
    "for i in featmap: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 2, 3)\n",
      "(1, 1, 1, 3)\n",
      "(1, 2, 2, 3)\n",
      "[[[[ 5 12 21]\n",
      "   [20 30 42]]\n",
      "\n",
      "  [[35 48 63]\n",
      "   [50 66 84]]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    matricesList = tf.constant([[[[1, 2, 3], [4, 5, 6]], \n",
    "                                 [[7, 8, 9], [10, 11, 12]]]])\n",
    "    print(matricesList.shape)\n",
    "    #matricesSum = tf.reduce_min(matricesList, axis=2)\n",
    "    #matricesSum = tf.reduce_min(matricesList, axis=2)\n",
    "    matricesSum = tf.reduce_mean(matricesList, axis=[1,2])\n",
    "    matricesSum = tf.expand_dims(matricesSum, 1)\n",
    "    matricesSum = tf.expand_dims(matricesSum, 2)\n",
    "\n",
    "    attention = tf.multiply(matricesList, matricesSum)\n",
    "    print(matricesSum.shape)\n",
    "    print(attention.shape)\n",
    "\n",
    "    print(sess.run(attention))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc16d5bf42dd3e4d4f2bdf6df5e9dafc63e21110c365c9158d2f6a63294047b3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('SAT': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
